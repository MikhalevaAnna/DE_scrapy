import argparse
import time
import uuid
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col,
    trim,
    when,
    lit,
    row_number,
    dense_rank,
    md5,
    current_timestamp,
    lower,
)
from pyspark.sql.window import Window
from books_scraper.books_scraper.settings import (
    POSTGRES_HOST,
    POSTGRES_PORT,
    POSTGRES_DB,
    POSTGRES_USER,
    POSTGRES_PASSWORD,
    MIN_QUALITY_VALUE,
)
from books_scraper.books_scraper.settings import POSTGRES_DB_URI
from sqlalchemy import create_engine, text
from datetime import datetime as dt

from etl_logger import setup_etl_logging
from database.schema_manager import SchemaManager


class BooksETL:
    def __init__(self, drop_existing=False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ETL-–ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∫–Ω–∏–≥.

        –°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–ø—É—Å–∫–∞, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–≥–≥–µ—Ä,
        –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç SparkSession –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö.

        Args:
            drop_existing (bool): –§–ª–∞–≥ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
                                —Ç–∞–±–ª–∏—Ü DWH –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º
        """
        self.start_time = dt.now()
        self.etl_run_id = (
            f"etl_run_{self.start_time.strftime('%Y%m%d_%H%M%S')}_"
            f"{uuid.uuid4().hex[:8]}"
        )
        self.drop_existing = drop_existing

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ª–æ–≥–≥–µ—Ä–∞
        self.etl_logger = setup_etl_logging("BooksETL")
        self.logger = self.etl_logger.logger

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        config = {
            "drop_existing": drop_existing,
            "dwh_schema": "dwh",
            "raw_schema": "public",
            "etl_run_id": self.etl_run_id,
        }
        self.etl_logger.log_etl_start(config)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats = {
            "extraction": {},
            "transformation": {},
            "quality": {},
            "load": {},
            "timing": {},
        }

        try:
            self.etl_logger.log_stage_start(
                "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark", "–°–æ–∑–¥–∞–Ω–∏–µ SparkSession"
            )

            # –°–æ–∑–¥–∞–µ–º SparkSession —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
            self.spark = (
                SparkSession.builder.appName(f"BooksETL_{self.etl_run_id}")
                .config("spark.jars.packages", "org.postgresql:postgresql:42.6.0")
                .config("spark.sql.execution.arrow.pyspark.enabled", "true")
                .config("spark.executor.memory", "2g")
                .config("spark.driver.memory", "2g")
                .getOrCreate()
            )

            app_id = self.spark.sparkContext.applicationId
            self.logger.info(f"‚úÖ SparkSession —Å–æ–∑–¥–∞–Ω. App ID: {app_id}")

            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
            self.db_properties = {
                "driver": "org.postgresql.Driver",
                "url": f"jdbc:postgresql://{POSTGRES_HOST}:{POSTGRES_PORT}"
                f"/{POSTGRES_DB}",
                "user": f"{POSTGRES_USER}",
                "password": f"{POSTGRES_PASSWORD}",
            }

            # –°—Ö–µ–º—ã
            self.raw_schema = "public"
            self.dwh_schema = "dwh"
            self.schema_manager = SchemaManager(drop_existing=self.drop_existing)

            self.etl_logger.log_stage_complete(
                "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark",
                {
                    "Spark Application ID": self.spark.sparkContext.applicationId,
                    "Spark Version": self.spark.version,
                },
            )

        except Exception as e:
            self.etl_logger.log_error(
                "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è", f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ETL: {str(e)}"
            )
            raise

    def initialize_dwh(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–∞–Ω–Ω—ã—Ö (DWH) —á–µ—Ä–µ–∑ SQLAlchemy.

        –°–æ–∑–¥–∞–µ—Ç —Å—Ö–µ–º—É DWH –∏ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ç–∞–±–ª–∏—Ü—ã (—Ñ–∞–∫—Ç–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã
        –∏ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏) —á–µ—Ä–µ–∑ SchemaManager.
        –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ª–æ–≥–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∏ –ø—Ä–µ–∫—Ä–∞—â–∞–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ.

        Returns:
            bool: True –µ—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞, –∏–Ω–∞—á–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
        """
        self.etl_logger.log_stage_start(
            "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DWH", "–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã –∏ —Ç–∞–±–ª–∏—Ü DWH"
        )

        try:
            start_time = time.time()
            success = self.schema_manager.initialize_dwh()
            duration = time.time() - start_time

            if success:
                self.etl_logger.log_stage_complete(
                    "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DWH",
                    {
                        "–°—Ç–∞—Ç—É—Å": "–£—Å–ø–µ—à–Ω–æ",
                        "–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è": f"{duration:.2f} —Å–µ–∫",
                        "–°—Ö–µ–º–∞": self.dwh_schema,
                    },
                )
            else:
                self.etl_logger.log_error(
                    "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DWH", "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å DWH"
                )
                raise Exception("Failed to initialize DWH")

            return success

        except Exception as e:
            self.etl_logger.log_error(
                "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DWH", f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ DWH: {str(e)}"
            )
            raise

    def extract_raw_data(self):
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è (raw layer).

        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã raw_books –≤ PostgreSQL —á–µ—Ä–µ–∑
        JDBC —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –∏ —Å–æ–±–∏—Ä–∞–µ—Ç
        —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.

        Returns:
            DataFrame: PySpark DataFrame —Å —Å—ã—Ä—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∫–Ω–∏–≥

        Raises:
            Exception: –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∏–ª–∏ —á—Ç–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        """
        try:
            start_time = time.time()

            raw_df = (
                self.spark.read.format("jdbc")
                .option("url", self.db_properties["url"])
                .option("dbtable", "raw_books")
                .option("user", self.db_properties["user"])
                .option("password", self.db_properties["password"])
                .option("driver", self.db_properties["driver"])
                .load()
            )

            count = raw_df.count()
            duration = time.time() - start_time

            # –£–ü–†–û–©–ï–ù–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê (–±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π)
            self.stats["extraction"] = {
                "records_extracted": count,
                "duration_seconds": duration,
            }

            self.logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π –∑–∞ {duration:.2f} —Å–µ–∫")

            if count == 0:
                self.logger.warning("–¢–∞–±–ª–∏—Ü–∞ raw_books –ø—É—Å—Ç–∞!")

            return raw_df

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
            raise

    def transform_data(self, raw_df):
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç DWH.

        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏:
        1. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤, –æ–±—Ä–∞–±–æ—Ç–∫–∞ NULL)
        2. –î–µ–¥—É–±–ª–∏–∫–∞—Ü–∏—è –ø–æ UPC –∫–æ–¥—É
        3. –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤ (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏, —Ç–∏–ø—ã –ø—Ä–æ–¥—É–∫—Ç–æ–≤, —Ä–µ–π—Ç–∏–Ω–≥–∏)
        4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã —Å –≤–Ω–µ—à–Ω–∏–º–∏ –∫–ª—é—á–∞–º–∏
        5. –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö

        Args:
            raw_df (DataFrame): DataFrame —Å —Å—ã—Ä—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∫–Ω–∏–≥

        Returns:
            dict: –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–º–∏ DataFrame:
                - "books": –§–∞–∫—Ç–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∫–Ω–∏–≥
                - "categories": –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
                - "product_types": –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Ç–∏–ø–æ–≤ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
                - "ratings": –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
        """
        self.etl_logger.log_stage_start(
            "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö", "–û—á–∏—Å—Ç–∫–∞, –¥–µ–¥—É–±–ª–∏–∫–∞—Ü–∏—è, —Å–æ–∑–¥–∞–Ω–∏–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤"
        )

        try:
            start_time = time.time()
            transformation_stats = {}
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è quality_details –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–æ–∫
            self.stats["quality_details"] = {"low_quality_count": 0}

            # 1. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            self.logger.info("–≠—Ç–∞–ø 1: –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
            cleaned_df = (
                raw_df.withColumn("title", trim(col("title")))
                .withColumn("category", trim(col("category")))
                .withColumn("product_type", trim(col("product_type")))
                .withColumn(
                    "price", when(col("price").isNull(), 0.0).otherwise(col("price"))
                )
                .withColumn(
                    "reviews_count",
                    when(col("reviews_count").isNull(), 0).otherwise(
                        col("reviews_count")
                    ),
                )
                .withColumn(
                    "rating", when(col("rating").isNull(), 0).otherwise(col("rating"))
                )
            )

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏
            null_counts = {}
            for field in ["title", "category", "product_type", "price", "rating"]:
                null_count = cleaned_df.filter(col(field).isNull()).count()
                null_counts[f"null_{field}"] = null_count

            transformation_stats["cleaning"] = null_counts

            # 2. –î–µ–¥—É–±–ª–∏–∫–∞—Ü–∏—è –ø–æ UPC
            self.logger.info("–≠—Ç–∞–ø 2: –î–µ–¥—É–±–ª–∏–∫–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")
            window_spec = Window.partitionBy("upc").orderBy("scraped_at")
            deduplicated_df = (
                cleaned_df.withColumn("row_num", row_number().over(window_spec))
                .filter(col("row_num") == 1)
                .drop("row_num")
            )

            total_before = cleaned_df.count()
            total_after = deduplicated_df.count()
            duplicates_removed = total_before - total_after

            transformation_stats["deduplication"] = {
                "records_before": total_before,
                "records_after": total_after,
                "duplicates_removed": duplicates_removed,
            }

            self.logger.info(f"  –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_removed}")

            # 3. –°–û–ó–î–ê–ù–ò–ï –°–ü–†–ê–í–û–ß–ù–ò–ö–û–í (DIMENSIONS)

            # 3.1 –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
            self.logger.info("–≠—Ç–∞–ø 3.1: –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
            categories_df = (
                deduplicated_df.filter(
                    (col("category").isNotNull()) & (col("category") != lit(""))
                )
                .select("category")
                .distinct()
                .withColumn("category_hash", md5(col("category")))
                .withColumn(
                    "category_id", dense_rank().over(Window.orderBy("category_hash"))
                )
                .select("category_id", "category")
                .withColumn("created_at", current_timestamp())
            )

            categories_count = categories_df.count()
            transformation_stats["categories"] = {"unique_categories": categories_count}

            # 3.2 –¢–∏–ø—ã –ø—Ä–æ–¥—É–∫—Ç–æ–≤
            self.logger.info("–≠—Ç–∞–ø 3.2: –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ —Ç–∏–ø–æ–≤ –ø—Ä–æ–¥—É–∫—Ç–æ–≤")
            product_types_df = (
                deduplicated_df.filter(
                    (col("product_type").isNotNull()) & (col("product_type") != lit(""))
                )
                .select("product_type")
                .distinct()
                .withColumn("product_type_hash", md5(col("product_type")))
                .withColumn(
                    "product_type_id",
                    dense_rank().over(Window.orderBy("product_type_hash")),
                )
                .select("product_type_id", "product_type")
                .withColumn("created_at", current_timestamp())
            )

            product_types_count = product_types_df.count()
            transformation_stats["product_types"] = {
                "unique_product_types": product_types_count
            }

            # 3.3 –†–µ–π—Ç–∏–Ω–≥–∏
            self.logger.info("–≠—Ç–∞–ø 3.3: –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤")
            try:
                ratings_df = (
                    self.spark.read.format("jdbc")
                    .option("url", self.db_properties["url"])
                    .option("dbtable", f"{self.dwh_schema}.dim_ratings")
                    .option("user", self.db_properties["user"])
                    .option("password", self.db_properties["password"])
                    .option("driver", self.db_properties["driver"])
                    .load()
                )

                ratings_count = ratings_df.count()
                self.logger.info(f"  –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –∏–∑ DWH: {ratings_count}")

                if ratings_count == 0:
                    self.logger.warning(
                        "–¢–∞–±–ª–∏—Ü–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –ø—É—Å—Ç–∞, —Å–æ–∑–¥–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è"
                    )
                    ratings_df = self.create_default_ratings()

            except Exception as e:
                self.logger.warning(
                    f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥–∏ "
                    f"–∏–∑ DWH: {e}, —Å–æ–∑–¥–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è"
                )
                ratings_df = self.create_default_ratings()

            # 4. –§–ê–ö–¢–û–í–ê–Ø –¢–ê–ë–õ–ò–¶–ê (FACT TABLE)
            self.logger.info("–≠—Ç–∞–ø 4: –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã")

            books_prepared_df = (
                deduplicated_df.filter(col("data_quality_score") >= MIN_QUALITY_VALUE)
                .withColumn(
                    "in_stock",
                    when(
                        lower(col("availability")).contains("in stock"), True
                    ).otherwise(False),
                )
                .withColumn(
                    "available_quantity",
                    when(col("available_quantity").isNull(), 0).otherwise(
                        col("available_quantity")
                    ),
                )
                .withColumn(
                    "availability_status",
                    when(lower(col("availability")).contains("in stock"), "in_stock")
                    .when(
                        lower(col("availability")).contains("out of stock"),
                        "out_of_stock",
                    )
                    .otherwise("unknown"),
                )
            )

            # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã —Å –≤–Ω–µ—à–Ω–∏–º–∏ –∫–ª—é—á–∞–º–∏
            books_df = (
                books_prepared_df.join(categories_df, "category", "left")
                .join(product_types_df, "product_type", "left")
                .join(
                    ratings_df,
                    books_prepared_df["rating"] == ratings_df["rating_value"],
                    "left",
                )
                .select(
                    col("upc").alias("book_id"),
                    col("title"),
                    col("description"),
                    col("price"),
                    col("in_stock"),
                    col("available_quantity"),
                    col("reviews_count"),
                    col("rating_id"),
                    col("category_id"),
                    col("product_type_id"),
                    col("image_url"),
                    col("url"),
                    col("scraped_at"),
                    current_timestamp().alias("processed_at"),
                )
            )

            # 4. –¢–ê–ë–õ–ò–¶–ê –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–•
            bad_quality_df = (
                deduplicated_df.filter(col("data_quality_score") < MIN_QUALITY_VALUE)
                .withColumn(
                    "in_stock",
                    when(
                        lower(col("availability")).contains("in stock"), True
                    ).otherwise(False),
                )
                .withColumn(
                    "available_quantity",
                    when(col("available_quantity").isNull(), 0).otherwise(
                        col("available_quantity")
                    ),
                )
                .withColumn(
                    "availability_status",
                    when(lower(col("availability")).contains("in stock"), "in_stock")
                    .when(
                        lower(col("availability")).contains("out of stock"),
                        "out_of_stock",
                    )
                    .otherwise("unknown"),
                )
            )

            books_count = books_df.count()
            bad_quality_count = bad_quality_df.count()
            transformation_stats["fact_table"] = {"total_books": books_count}
            transformation_stats["bad_quality_books"] = bad_quality_count
            # 5. –†–ê–°–ß–ï–¢ –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–•

            self.logger.info("–≠—Ç–∞–ø 5: –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–Ω–∏–≥–∞–º")
            self.analyze_data_quality_details(bad_quality_df)

            duration = time.time() - start_time
            transformation_stats["duration_seconds"] = round(duration, 2)

            self.stats["transformation"] = transformation_stats

            self.etl_logger.log_stage_complete(
                "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö",
                {
                    "–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è": f"{duration:.2f} —Å–µ–∫",
                    "–í—Å–µ–≥–æ –∫–Ω–∏–≥ –ø–æ—Å–ª–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏": books_count,
                    "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π": categories_count,
                    "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–æ–¥—É–∫—Ç–æ–≤": product_types_count,
                },
            )

            return {
                "books": books_df,
                "categories": categories_df,
                "product_types": product_types_df,
                "ratings": ratings_df,
            }

        except Exception as e:
            self.etl_logger.log_error(
                "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö", f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {str(e)}"
            )
            raise

    def analyze_data_quality_details(self, low_quality_books):
        """
        –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–Ω–∏–≥ —Å –Ω–∏–∑–∫–∏–º score.

        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–Ω–∏–≥–∏ —Å data_quality_score –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è,
        –ª–æ–≥–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–ø–∏—Å—è—Ö –∏ —Å–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–±–ª–µ–º —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.

        Args:
            low_quality_books (DataFrame): DataFrame —Å –∫–Ω–∏–≥–∞–º–∏,
                                          –∏–º–µ—é—â–∏–º–∏ –Ω–∏–∑–∫–∏–π quality score

        Returns:
            DataFrame: –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame –¥–ª—è —Ü–µ–ø–æ—á–∫–∏ –≤—ã–∑–æ–≤–æ–≤
        """
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            if "quality_details" not in self.stats:
                self.stats["quality_details"] = {"low_quality_count": 0}

            self.logger.info("üîç –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–Ω–∏–≥–∞–º...")

            # –ü–æ–ª—É—á–∞–µ–º –∫–Ω–∏–≥–∏ —Å –Ω–∏–∑–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö (< 85%)
            low_quality_count = low_quality_books.count()

            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.logger.info("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–•:")

            # 1. –ö–Ω–∏–≥–∏ —Å –Ω–∏–∑–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º (<85%)
            if low_quality_count > 0:
                self.logger.warning(
                    f"‚ùå‚ùå‚ùå –ù–ê–ô–î–ï–ù–´ –ö–ù–ò–ì–ò "
                    f"–° –ù–ò–ó–ö–ò–ú –ö–ê–ß–ï–°–¢–í–û–ú –î–ê–ù–ù–´–• "
                    f"(<{MIN_QUALITY_VALUE}%):"
                )
                self.logger.info(f"  ‚ö†Ô∏è  –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —É {low_quality_count} –∫–Ω–∏–≥")

                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ book_id –∫–Ω–∏–≥ —Å –Ω–∏–∑–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º
                low_quality_ids = low_quality_books.select("upc").collect()
                low_quality_ids_list = [row["upc"] for row in low_quality_ids]

                # –í—ã–≤–æ–¥–∏–º –≤—Å–µ book_id
                self.logger.warning(
                    f"    üìã UPC –≤—Å–µ—Ö –∫–Ω–∏–≥ —Å –Ω–∏–∑–∫–∏–º "
                    f"–∫–∞—á–µ—Å—Ç–≤–æ–º —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"
                    f": {', '.join(low_quality_ids_list)}"
                )

                # –í—ã–≤–æ–¥–∏–º –¥–µ—Ç–∞–ª–∏ –ø–æ –ø–µ—Ä–≤—ã–º 10 –∫–Ω–∏–≥–∞–º
                low_quality_details = (
                    low_quality_books.select("upc", "title", "data_quality_score")
                    .orderBy("data_quality_score")
                    .limit(10)
                    .collect()
                )

                self.logger.warning("    –ü—Ä–∏–º–µ—Ä—ã –∫–Ω–∏–≥ —Å –Ω–∏–∑–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º:")
                for book in low_quality_details:
                    title = book["title"] if book["title"] else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
                    self.logger.warning(
                        f"      - {book['upc']}: '{title[:50]}...' - "
                        f"{book['data_quality_score']:.1f}%"
                    )

                if low_quality_count > 10:
                    self.logger.warning(
                        f"      ... –∏ –µ—â–µ {low_quality_count - 10} –∫–Ω–∏–≥"
                    )

            else:
                self.logger.info(
                    f"‚úÖ –ù–µ—Ç –∫–Ω–∏–≥ —Å –∫–∞—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö –Ω–∏–∂–µ {MIN_QUALITY_VALUE}%"
                )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.stats["quality_details"]["low_quality_count"] = low_quality_count

            return low_quality_books

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ—Ç–∞–ª—å–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")
            # –í—Å–µ —Ä–∞–≤–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.stats["quality_details"] = {"low_quality_count": 0}
            return low_quality_books

    def create_default_ratings(self):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.

        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –±–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –æ—Ç 0 –¥–æ 5 –∑–≤–µ–∑–¥ —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ —Ç–∞–±–ª–∏—Ü–∞ dim_ratings –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–∞.

        Returns:
            DataFrame: PySpark DataFrame —Å–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–º —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
        """
        self.logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")

        ratings_df = self.spark.createDataFrame(
            [
                (1, 0, "Zero", "–ë–µ–∑ —Ä–µ–π—Ç–∏–Ω–≥–∞"),
                (2, 1, "One", "–û—á–µ–Ω—å –ø–ª–æ—Ö–æ"),
                (3, 2, "Two", "–ü–ª–æ—Ö–æ"),
                (4, 3, "Three", "–°—Ä–µ–¥–Ω–µ"),
                (5, 4, "Four", "–•–æ—Ä–æ—à–æ"),
                (6, 5, "Five", "–û—Ç–ª–∏—á–Ω–æ"),
            ],
            ["rating_id", "rating_value", "rating_name", "rating_description"],
        )

        self.logger.info(f"–°–æ–∑–¥–∞–Ω–æ {ratings_df.count()} –∑–∞–ø–∏—Å–µ–π —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        return ratings_df

    def load_to_dwh(self, dataframes):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Å–ª–æ–π —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–∞–Ω–Ω—ã—Ö (DWH).

        –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü—ã DWH –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ:
        1. –û—á–∏—Å—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü
        2. –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤ (dimensions)
        3. –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–∫—Ç–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã
        4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∑–∫–∏

        Args:
            dataframes (dict): –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–º–∏ DataFrame

        Returns:
            dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º
        """
        self.etl_logger.log_stage_start(
            "–ó–∞–≥—Ä—É–∑–∫–∞ –≤ DWH", "–ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ DWH"
        )

        try:
            load_start_time = time.time()
            load_stats = {}

            # 1. –û—á–∏—â–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã —á–µ—Ä–µ–∑ SQLAlchemy
            self.logger.info("–®–∞–≥ 1: –û—á–∏—Å—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü DWH")
            try:
                engine = create_engine(POSTGRES_DB_URI)
                with engine.connect() as conn:
                    truncate_sql = f"""
                        TRUNCATE TABLE
                            {self.dwh_schema}.fact_books,
                            {self.dwh_schema}.dim_product_types,
                            {self.dwh_schema}.dim_categories
                        CASCADE;
                    """

                    try:
                        conn.execute(text(truncate_sql))
                        self.logger.info("‚úì –¢–∞–±–ª–∏—Ü—ã –æ—á–∏—â–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é TRUNCATE CASCADE")
                    except Exception as e:
                        self.logger.warning(f"TRUNCATE CASCADE –Ω–µ —É–¥–∞–ª—Å—è: {e}")
                        self.delete_tables_in_order(conn)

                    conn.commit()

            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Ç–∞–±–ª–∏—Ü: {e}")

            # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ append —Ä–µ–∂–∏–º–µ
            load_stats["tables_loaded"] = {}

            # –ó–∞–≥—Ä—É–∑–∫–∞ dim_categories
            self.logger.info("–®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ dim_categories")
            categories_count = dataframes["categories"].count()
            categories_start = time.time()
            dataframes["categories"].write.mode("append").jdbc(
                url=self.db_properties["url"],
                table=f"{self.dwh_schema}.dim_categories",
                properties=self.db_properties,
            )
            categories_duration = time.time() - categories_start
            load_stats["tables_loaded"]["dim_categories"] = {
                "records": categories_count,
                "duration_seconds": round(categories_duration, 2),
            }
            self.etl_logger.log_data_load(
                "dim_categories", categories_count, categories_duration
            )

            # –ó–∞–≥—Ä—É–∑–∫–∞ dim_product_types
            self.logger.info("–®–∞–≥ 3: –ó–∞–≥—Ä—É–∑–∫–∞ dim_product_types")
            product_types_count = dataframes["product_types"].count()
            product_types_start = time.time()
            dataframes["product_types"].write.mode("append").jdbc(
                url=self.db_properties["url"],
                table=f"{self.dwh_schema}.dim_product_types",
                properties=self.db_properties,
            )
            product_types_duration = time.time() - product_types_start
            load_stats["tables_loaded"]["dim_product_types"] = {
                "records": product_types_count,
                "duration_seconds": round(product_types_duration, 2),
            }
            self.etl_logger.log_data_load(
                "dim_product_types", product_types_count, product_types_duration
            )

            # –ó–∞–≥—Ä—É–∑–∫–∞ dim_ratings (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—É—Å—Ç–∞—è)
            self.logger.info("–®–∞–≥ 4: –ó–∞–≥—Ä—É–∑–∫–∞ dim_ratings")
            try:
                existing_ratings = (
                    self.spark.read.format("jdbc")
                    .option("url", self.db_properties["url"])
                    .option("dbtable", f"{self.dwh_schema}.dim_ratings")
                    .option("user", self.db_properties["user"])
                    .option("password", self.db_properties["password"])
                    .option("driver", self.db_properties["driver"])
                    .load()
                )

                if existing_ratings.count() == 0:
                    ratings_count = dataframes["ratings"].count()
                    ratings_start = time.time()
                    dataframes["ratings"].write.mode("append").jdbc(
                        url=self.db_properties["url"],
                        table=f"{self.dwh_schema}.dim_ratings",
                        properties=self.db_properties,
                    )
                    ratings_duration = time.time() - ratings_start
                    load_stats["tables_loaded"]["dim_ratings"] = {
                        "records": ratings_count,
                        "duration_seconds": round(ratings_duration, 2),
                        "status": "loaded",
                    }
                    self.etl_logger.log_data_load(
                        "dim_ratings", ratings_count, ratings_duration
                    )
                else:
                    self.logger.info(
                        "‚úì –¢–∞–±–ª–∏—Ü–∞ dim_ratings —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É"
                    )
                    load_stats["tables_loaded"]["dim_ratings"] = {
                        "records": existing_ratings.count(),
                        "status": "skipped (already has data)",
                    }

            except Exception as e:
                self.logger.warning(
                    f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å dim_ratings: {e}, –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ"
                )
                ratings_count = dataframes["ratings"].count()
                ratings_start = time.time()
                dataframes["ratings"].write.mode("append").jdbc(
                    url=self.db_properties["url"],
                    table=f"{self.dwh_schema}.dim_ratings",
                    properties=self.db_properties,
                )
                ratings_duration = time.time() - ratings_start
                load_stats["tables_loaded"]["dim_ratings"] = {
                    "records": ratings_count,
                    "duration_seconds": round(ratings_duration, 2),
                    "status": "loaded (fallback)",
                }
                self.etl_logger.log_data_load(
                    "dim_ratings", ratings_count, ratings_duration
                )

            # –ó–∞–≥—Ä—É–∑–∫–∞ fact_books
            self.logger.info("–®–∞–≥ 5: –ó–∞–≥—Ä—É–∑–∫–∞ fact_books")
            books_count = dataframes["books"].count()
            books_start = time.time()
            dataframes["books"].write.mode("append").jdbc(
                url=self.db_properties["url"],
                table=f"{self.dwh_schema}.fact_books",
                properties=self.db_properties,
            )
            books_duration = time.time() - books_start
            load_stats["tables_loaded"]["fact_books"] = {
                "records": books_count,
                "duration_seconds": round(books_duration, 2),
            }
            self.etl_logger.log_data_load("fact_books", books_count, books_duration)

            # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏
            self.logger.info("–®–∞–≥ 6: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
            verification_stats = self.verify_data_load()
            load_stats["verification"] = verification_stats

            total_duration = time.time() - load_start_time
            load_stats["total_duration_seconds"] = round(total_duration, 2)

            self.stats["load"] = load_stats

            self.etl_logger.log_stage_complete(
                "–ó–∞–≥—Ä—É–∑–∫–∞ –≤ DWH",
                {
                    "–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è": f"{total_duration:.2f} —Å–µ–∫",
                    "–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–∞–±–ª–∏—Ü": len(load_stats["tables_loaded"]),
                    "–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π fact_books": books_count,
                },
            )

            return load_stats

        except Exception as e:
            self.etl_logger.log_error(
                "–ó–∞–≥—Ä—É–∑–∫–∞ –≤ DWH", f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {str(e)}"
            )
            raise

    def delete_tables_in_order(self, conn):
        """
        –†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥ —É–¥–∞–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–∞–±–ª–∏—Ü DWH.

        –£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ –∏–∑ —Ç–∞–±–ª–∏—Ü –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:
        1. fact_books (—Ñ–∞–∫—Ç–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞)
        2. dim_product_types (—Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Ç–∏–ø–æ–≤ –ø—Ä–æ–¥—É–∫—Ç–æ–≤)
        3. dim_categories (—Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π)

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏ –Ω–µ—É–¥–∞—á–Ω–æ–π –ø–æ–ø—ã—Ç–∫–µ TRUNCATE CASCADE.

        Args:
            conn: SQLAlchemy connection object
        """
        delete_queries = [
            f"DELETE FROM {self.dwh_schema}.fact_books",
            f"DELETE FROM {self.dwh_schema}.dim_product_types",
            f"DELETE FROM {self.dwh_schema}.dim_categories",
        ]

        for query in delete_queries:
            try:
                conn.execute(text(query))
                self.logger.debug(f"–í—ã–ø–æ–ª–Ω–µ–Ω: {query}")
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ {query}: {e}")

    def verify_data_load(self):
        """
        –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ DWH.

        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–æ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü–∞—Ö DWH –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏,
        –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –∏ –ª–æ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
        –ü–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—è–≤–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –Ω–∞ —Ä–∞–Ω–Ω–µ–º —ç—Ç–∞–ø–µ.

        Returns:
            dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ –∫–∞–∂–¥–æ–π —Ç–∞–±–ª–∏—Ü–µ
        """
        self.logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ DWH...")

        verification_stats = {}

        try:
            tables_to_check = [
                ("dim_ratings", "–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤"),
                ("dim_categories", "–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"),
                ("dim_product_types", "–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Ç–∏–ø–æ–≤ –ø—Ä–æ–¥—É–∫—Ç–æ–≤"),
                ("fact_books", "–§–∞–∫—Ç–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∫–Ω–∏–≥"),
            ]

            for table_name, table_description in tables_to_check:
                try:
                    df = (
                        self.spark.read.format("jdbc")
                        .option("url", self.db_properties["url"])
                        .option("dbtable", f"{self.dwh_schema}.{table_name}")
                        .option("user", self.db_properties["user"])
                        .option("password", self.db_properties["password"])
                        .option("driver", self.db_properties["driver"])
                        .load()
                    )

                    count = df.count()
                    verification_stats[table_name] = {
                        "records": count,
                        "status": "loaded" if count > 0 else "empty",
                    }

                    self.logger.info(f"  ‚úì {table_description}: {count} –∑–∞–ø–∏—Å–µ–π")

                    if count == 0:
                        self.logger.warning(f"  ‚ö†Ô∏è  –¢–∞–±–ª–∏—Ü–∞ {table_name} –ø—É—Å—Ç–∞!")

                except Exception as e:
                    verification_stats[table_name] = {
                        "records": 0,
                        "status": "error",
                        "error": str(e)[:100],
                    }
                    self.logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã {table_name}: {e}")

            return verification_stats

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return {"error": str(e)}

    def run(self):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ ETL-–ø–∞–π–ø–ª–∞–π–Ω–∞.

        –û—Ä–∫–µ—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤ ETL:
        1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DWH
        2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        3. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        4. –ó–∞–≥—Ä—É–∑–∫–∞ –≤ DWH
        5. –°–±–æ—Ä —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏

        Returns:
            dict: –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è ETL

        Raises:
            Exception: –ü—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–∫–∞—Ö –≤ –ª—é–±–æ–º –∏–∑ —ç—Ç–∞–ø–æ–≤
        """
        try:
            self.etl_logger.log_stage_start(
                "–ó–∞–ø—É—Å–∫ ETL –ø—Ä–æ—Ü–µ—Å—Å–∞", f"ETL Run ID: {self.etl_run_id}"
            )

            # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DWH (—Å–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã –∏ —Ç–∞–±–ª–∏—Ü)
            self.initialize_dwh()

            # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ raw —Å–ª–æ—è
            raw_data = self.extract_raw_data()

            if raw_data.count() == 0:
                self.etl_logger.log_error("–ó–∞–ø—É—Å–∫ ETL", "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                self.logger.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏! –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
                return

            # 3. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            transformed_data = self.transform_data(raw_data)

            # 4. –ó–∞–≥—Ä—É–∑–∫–∞ –≤ DWH —Å–ª–æ–π
            self.load_to_dwh(transformed_data)

            # 5. –°–±–æ—Ä –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            total_duration = (dt.now() - self.start_time).total_seconds()

            total_stats = {
                "etl_run_id": self.etl_run_id,
                "total_duration_seconds": round(total_duration, 2),
                "extraction_records": (
                    self.stats["extraction"].get("records_extracted", 0)
                ),
                "transformation_books": (
                    self.stats["transformation"]
                    .get("fact_table", {})
                    .get("total_books", 0)
                ),
                "quality": self.stats["quality_details"].get("low_quality_count", {}),
                "tables_loaded": len(self.stats["load"].get("tables_loaded", {})),
                "spark_application_id": self.spark.sparkContext.applicationId,
                "status": "success",
            }

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –ª–æ–≥–≥–µ—Ä
            self.etl_logger.add_statistic("total_stats", total_stats)

            self.etl_logger.log_completion(True, total_stats)
            self.logger.info("üéâ ETL –ø—Ä–æ—Ü–µ—Å—Å —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")

            return total_stats

        except Exception as e:
            total_duration = (dt.now() - self.start_time).total_seconds()

            error_stats = {
                "etl_run_id": self.etl_run_id,
                "total_duration_seconds": round(total_duration, 2),
                "status": "failed",
                "error": str(e)[:200],
            }

            self.etl_logger.log_completion(False, error_stats)
            self.etl_logger.log_error(
                "–ó–∞–ø—É—Å–∫ ETL", f"ETL –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π: {str(e)}"
            )

            raise

        finally:
            try:
                self.spark.stop()
                self.logger.info("‚úÖ Spark —Å–µ—Å—Å–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ Spark —Å–µ—Å—Å–∏–∏: {e}")


if __name__ == "__main__":
    """
    –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ ETL-–ø–∞–π–ø–ª–∞–π–Ω–∞ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏.

    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç
    –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç ETL –ø—Ä–æ—Ü–µ—Å—Å, –∞ —Ç–∞–∫–∂–µ —É–ø—Ä–∞–≤–ª—è–µ—Ç –∫–æ–¥–æ–º –≤–æ–∑–≤—Ä–∞—Ç–∞
    –ø—Ä–æ–≥—Ä–∞–º–º—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏:
        --drop: –£–¥–∞–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–∞–±–ª–∏—Ü—ã DWH –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º
        --verbose (-v): –í–∫–ª—é—á–∞–µ—Ç –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

    Exit codes:
        0: –£—Å–ø–µ—à–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
        1: –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è ETL
    """
    parser = argparse.ArgumentParser(description="Run Books ETL Pipeline")
    parser.add_argument(
        "--drop", action="store_true", help="Drop existing DWH tables before run"
    )
    parser.add_argument(
        "--verbose", "-v", action="store_true", help="Enable verbose logging"
    )

    args = parser.parse_args()

    # –ó–∞–ø—É—Å–∫ ETL
    try:
        etl = BooksETL(drop_existing=args.drop)
        result = etl.run()

        if result and result.get("status") == "success":
            print("\n‚úÖ ETL —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")
            print(f"   Run ID: {result['etl_run_id']}")
            print(f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {result['total_duration_seconds']} —Å–µ–∫")
            print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–Ω–∏–≥: {result['transformation_books']}")
            exit(0)
        else:
            print("\n‚ùå ETL –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π")
            exit(1)

    except Exception as e:
        print(f"\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ ETL: {e}")
        exit(1)
