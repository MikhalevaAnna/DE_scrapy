import random
import time
from scrapy import signals
import logging


class RandomDelayMiddleware:
    """
    Middleware –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–ª—É—á–∞–π–Ω—ã—Ö –∑–∞–¥–µ—Ä–∂–µ–∫ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ —Å
    –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
    """

    def __init__(self, delay_range):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è middleware —Å–ª—É—á–∞–π–Ω—ã—Ö –∑–∞–¥–µ—Ä–∂–µ–∫.

        Args:
            delay_range (tuple): –î–∏–∞–ø–∞–∑–æ–Ω –∑–∞–¥–µ—Ä–∂–µ–∫ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (min, max)
        """
        self.delay_range = delay_range
        self.logger = None  # –ë—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø—Ä–∏ spider_opened
        self.total_delay = 0.0
        self.request_count = 0
        self.delays_by_url = {}  # –î–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ URL

    @classmethod
    def from_crawler(cls, crawler):
        """
        –§–∞–±—Ä–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ middleware –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ Crawler.

        –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∑–∞–¥–µ—Ä–∂–µ–∫ –∏–∑ settings.py –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
        —Å–∏–≥–Ω–∞–ª–æ–≤ spider_opened –∏ spider_closed.

        Args:
            crawler: –û–±—ä–µ–∫—Ç Scrapy Crawler

        Returns:
            RandomDelayMiddleware: –≠–∫–∑–µ–º–ø–ª—è—Ä middleware
        """
        delay_range = crawler.settings.get("DOWNLOAD_DELAY_RANGE", (0.5, 2.0))

        # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—É—á–µ–Ω–æ –∫–∞–∫ —Å–ø–∏—Å–æ–∫, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ tuple
        if isinstance(delay_range, list):
            delay_range = tuple(delay_range)

        middleware = cls(delay_range)

        # –ü–æ–¥–∫–ª—é—á–∞–µ–º —Å–∏–≥–Ω–∞–ª—ã
        crawler.signals.connect(middleware.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(middleware.spider_closed, signal=signals.spider_closed)

        return middleware

    def spider_opened(self, spider):
        """
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –æ—Ç–∫—Ä—ã—Ç–∏–∏ –ø–∞—É–∫–∞ (–Ω–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞).

        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–≥–≥–µ—Ä –∏ –ª–æ–≥–∏—Ä—É–µ—Ç –≤—Å–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏:
        - –î–∏–∞–ø–∞–∑–æ–Ω —Å–ª—É—á–∞–π–Ω—ã—Ö –∑–∞–¥–µ—Ä–∂–µ–∫
        - –ë–∞–∑–æ–≤—É—é –∑–∞–¥–µ—Ä–∂–∫—É Scrapy
        - –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–≤—Ç–æ—Ç—Ä–æ—Ç—Ç–ª–∏–Ω–≥–∞ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)

        Args:
            spider: –û–±—ä–µ–∫—Ç Scrapy Spider
        """
        # –°–æ–∑–¥–∞–µ–º –ª–æ–≥–≥–µ—Ä —Å –∏–º–µ–Ω–µ–º –ø–∞—É–∫–∞.middlewares
        self.logger = logging.getLogger(f"{spider.name}.middlewares")

        # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        self.logger.info("=" * 50)
        self.logger.info("üéØ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø RANDOM DELAY MIDDLEWARE")
        self.logger.info("=" * 50)
        self.logger.info(
            f"–î–∏–∞–ø–∞–∑–æ–Ω –∑–∞–¥–µ—Ä–∂–µ–∫: "
            f"{self.delay_range[0]:.2f} - {self.delay_range[1]:.2f} —Å–µ–∫"
        )
        self.logger.info(
            f"–ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: {spider.settings.get('DOWNLOAD_DELAY', 0):.2f} —Å–µ–∫"
        )
        self.logger.info(
            f"–°–ª—É—á–∞–π–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏: "
            f"{spider.settings.get('RANDOMIZE_DOWNLOAD_DELAY', True)}"
        )
        self.logger.info(
            f"–ê–≤—Ç–æ—Ç—Ä–æ—Ç—Ç–ª–∏–Ω–≥: {spider.settings.get('AUTOTHROTTLE_ENABLED', False)}"
        )

        if spider.settings.get("AUTOTHROTTLE_ENABLED"):
            self.logger.info(
                f"  –ù–∞—á–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: "
                f"{spider.settings.get('AUTOTHROTTLE_START_DELAY', 5.0)} —Å–µ–∫"
            )
            self.logger.info(
                f"  –ú–∞–∫—Å. –∑–∞–¥–µ—Ä–∂–∫–∞: "
                f"{spider.settings.get('AUTOTHROTTLE_MAX_DELAY', 60.0)} —Å–µ–∫"
            )

        self.logger.info("=" * 50)

    def spider_closed(self, spider):
        """
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ä–∞–±–æ—Ç—ã –ø–∞—É–∫–∞.

        –í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∑–∞–¥–µ—Ä–∂–∫–∞–º:
        - –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
        - –°—É–º–º–∞—Ä–Ω–æ–µ –≤—Ä–µ–º—è –≤—Å–µ—Ö –∑–∞–¥–µ—Ä–∂–µ–∫
        - –°—Ä–µ–¥–Ω—é—é –∑–∞–¥–µ—Ä–∂–∫—É –Ω–∞ –∑–∞–ø—Ä–æ—Å
        - –¢–æ–ø-5 —Å–∞–º—ã—Ö –¥–æ–ª–≥–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

        Args:
            spider: –û–±—ä–µ–∫—Ç Scrapy Spider
        """
        try:
            if self.request_count > 0:
                avg_delay = self.total_delay / self.request_count

                self.logger.info("=" * 50)
                self.logger.info("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ó–ê–î–ï–†–ñ–ï–ö")
                self.logger.info("=" * 50)
                self.logger.info(f"–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {self.request_count}")
                self.logger.info(f"–û–±—â–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: {self.total_delay:.2f} —Å–µ–∫")
                self.logger.info(f"–°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞: {avg_delay:.2f} —Å–µ–∫")

                if self.delays_by_url:
                    min_delay = min(self.delays_by_url.values())
                    max_delay = max(self.delays_by_url.values())
                    self.logger.info(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: {min_delay:.2f} —Å–µ–∫")
                    self.logger.info(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: {max_delay:.2f} —Å–µ–∫")

                    # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ø-5 —Å–∞–º—ã—Ö –¥–æ–ª–≥–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                    sorted_delays = sorted(
                        self.delays_by_url.items(), key=lambda x: x[1], reverse=True
                    )[:5]
                    self.logger.info("–¢–æ–ø-5 —Å–∞–º—ã—Ö –¥–æ–ª–≥–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:")
                    for url, delay in sorted_delays:
                        self.logger.info(f"  {delay:.2f} —Å–µ–∫: {url[:80]}...")
                else:
                    self.logger.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –∑–∞–¥–µ—Ä–∂–∫–∞—Ö –ø–æ URL")

                self.logger.info("=" * 50)
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–∞–¥–µ—Ä–∂–µ–∫: {e}")

    def process_request(self, request, spider):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∂–¥—ã–π –∏—Å—Ö–æ–¥—è—â–∏–π –∑–∞–ø—Ä–æ—Å, –¥–æ–±–∞–≤–ª—è—è —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É.

        –õ–æ–≥–∏–∫–∞ —Ä–∞–±–æ—Ç—ã:
        1. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–∞–¥–µ—Ä–∂–∫—É –∏–∑ meta['download_delay'] –∏–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω—É—é
        2. –ü—Ä–∏–º–µ–Ω—è–µ—Ç –∑–∞–¥–µ—Ä–∂–∫—É —á–µ—Ä–µ–∑ time.sleep()
        3. –°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∑–∞–¥–µ—Ä–∂–∫–∞–º
        4. –õ–æ–≥–∏—Ä—É–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 50 –∑–∞–ø—Ä–æ—Å–æ–≤

        Args:
            request: Scrapy Request –æ–±—ä–µ–∫—Ç
            spider: –û–±—ä–µ–∫—Ç Scrapy Spider

        Returns:
            None: –ü–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–ø—Ä–æ—Å–∞
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—É—é
            delay = request.meta.get(
                "download_delay", random.uniform(*self.delay_range)
            )

            # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏ –∑–∞–¥–µ—Ä–∂–∫–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è DEBUG –∏–ª–∏ –∫–∞–∂–¥—É—é 20-—é)
            if self.request_count % 20 == 0:
                self.logger.info(
                    f"‚è±Ô∏è  –ó–∞–¥–µ—Ä–∂–∫–∞ {delay:.2f} —Å–µ–∫ –¥–ª—è {request.url[:60]}... "
                    f"(–º–µ—Ç–∞: {request.meta.get('download_delay', '–≥–µ–Ω–µ—Ä–∞—Ü–∏—è')})"
                )
            else:
                self.logger.debug(f"–ó–∞–¥–µ—Ä–∂–∫–∞ {delay:.2f} —Å–µ–∫ –¥–ª—è {request.url[:60]}...")

            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
            time.sleep(delay)

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.total_delay += delay
            self.request_count += 1

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è UR
            url_key = (
                request.url.split("/")[-1] if len(request.url) > 50 else request.url
            )
            self.delays_by_url[url_key] = delay

            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 50 –∑–∞–ø—Ä–æ—Å–æ–≤
            if self.request_count % 50 == 0:
                avg_delay = self.total_delay / self.request_count
                self.logger.info(
                    f"[–ü—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–¥–µ—Ä–∂–µ–∫] –ó–∞–ø—Ä–æ—Å–æ–≤: {self.request_count}, "
                    f"–û–±—â–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: {self.total_delay:.1f} —Å–µ–∫, "
                    f"–°—Ä–µ–¥–Ω—è—è: {avg_delay:.2f} —Å–µ–∫"
                )

            return None
        except Exception as e:
            # –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞, –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            spider.logger.error(f"–û—à–∏–±–∫–∞ –≤ RandomDelayMiddleware: {e}")
            return None


class SeleniumLoggingMiddleware:
    """Middleware –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Selenium –∑–∞–ø—Ä–æ—Å–æ–≤."""

    def __init__(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è middleware –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è Selenium.

        –°–æ–∑–¥–∞–µ—Ç —Å—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è Selenium –∏ –æ–±—ã—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.
        """
        self.logger = None
        self.selenium_requests = 0
        self.regular_requests = 0

    @classmethod
    def from_crawler(cls, crawler):
        """
        –§–∞–±—Ä–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ middleware.

        –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–∏–≥–Ω–∞–ª–æ–≤ spider_opened –∏ spider_closed.

        Args:
            crawler: –û–±—ä–µ–∫—Ç Scrapy Crawler

        Returns:
            SeleniumLoggingMiddleware: –≠–∫–∑–µ–º–ø–ª—è—Ä middleware
        """
        middleware = cls()
        crawler.signals.connect(middleware.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(middleware.spider_closed, signal=signals.spider_closed)
        return middleware

    def spider_opened(self, spider):
        """
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –æ—Ç–∫—Ä—ã—Ç–∏–∏ –ø–∞—É–∫–∞.

        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ –ª–æ–≥–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Selenium –∏–∑ settings.py:
        - –í–∫–ª—é—á–µ–Ω –ª–∏ Selenium
        - –¢–∏–ø –¥—Ä–∞–π–≤–µ—Ä–∞ (Chrome/Firefox)
        - Headless —Ä–µ–∂–∏–º
        - –ê—Ä–≥—É–º–µ–Ω—Ç—ã –¥—Ä–∞–π–≤–µ—Ä–∞

        Args:
            spider: –û–±—ä–µ–∫—Ç Scrapy Spider
        """
        self.logger = logging.getLogger(f"{spider.name}.middlewares.selenium")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Selenium
        selenium_enabled = spider.settings.get("SELENIUM_DRIVER_NAME") is not None

        self.logger.info("=" * 50)
        self.logger.info("üîß –ù–ê–°–¢–†–û–ô–ö–ò SELENIUM")
        self.logger.info("=" * 50)
        self.logger.info(f"Selenium –≤–∫–ª—é—á–µ–Ω: {selenium_enabled}")

        if selenium_enabled:
            driver_name = spider.settings.get("SELENIUM_DRIVER_NAME", "chrome")
            driver_args = spider.settings.getlist("SELENIUM_DRIVER_ARGUMENTS", [])
            headless = "--headless" in driver_args

            self.logger.info(f"–î—Ä–∞–π–≤–µ—Ä: {driver_name}")
            self.logger.info(f"Headless —Ä–µ–∂–∏–º: {headless}")
            self.logger.info(f"–ê—Ä–≥—É–º–µ–Ω—Ç—ã: {driver_args[:3]}...")
        else:
            self.logger.warning("Selenium –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ settings.py")

        self.logger.info("=" * 50)

    def spider_closed(self, spider):
        """
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ä–∞–±–æ—Ç—ã –ø–∞—É–∫–∞.

        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ –ª–æ–≥–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Selenium:
        - –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
        - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ Selenium vs –æ–±—ã—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        - –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤

        Args:
            spider: –û–±—ä–µ–∫—Ç Scrapy Spider
        """
        try:
            total_requests = self.selenium_requests + self.regular_requests

            if total_requests > 0:
                selenium_percent = (self.selenium_requests / total_requests) * 100

                self.logger.info("=" * 50)
                self.logger.info("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê SELENIUM")
                self.logger.info("=" * 50)
                self.logger.info(f"–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {total_requests}")
                self.logger.info(
                    f"Selenium –∑–∞–ø—Ä–æ—Å–æ–≤: "
                    f"{self.selenium_requests} "
                    f"({selenium_percent:.1f}%)"
                )
                self.logger.info(
                    f"–û–±—ã—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {self.regular_requests} "
                    f"({100 - selenium_percent:.1f}%)"
                )

                if self.selenium_requests > 0:
                    self.logger.info(
                        "üí° –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: Selenium –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è "
                        "–¥–ª—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–Ω–∏–≥"
                    )
                    self.logger.info(
                        "   –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º"
                    )

                self.logger.info("=" * 50)
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ Selenium: {e}")

    def process_request(self, request, spider):
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ (Selenium/–æ–±—ã—á–Ω—ã–π).

        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∞—Ç—Ä–∏–±—É—Ç—ã –∑–∞–ø—Ä–æ—Å–∞, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ–Ω
        SeleniumRequest. –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º –∑–∞–ø—Ä–æ—Å–æ–≤.

        Args:
            request: Scrapy Request –æ–±—ä–µ–∫—Ç
            spider: –û–±—ä–µ–∫—Ç Scrapy Spider

        Returns:
            None: –ü–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–ø—Ä–æ—Å–∞
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ SeleniumRequest
            is_selenium = hasattr(request, "wait_time") or "selenium" in request.meta

            if is_selenium:
                self.selenium_requests += 1
                # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—É—é 10-—é Selenium –∑–∞–ø—Ä–æ—Å
                if self.selenium_requests % 10 == 0:
                    wait_time = getattr(request, "wait_time", "N/A")
                    self.logger.info(
                        f"üöó Selenium –∑–∞–ø—Ä–æ—Å #{self.selenium_requests}: "
                        f"{request.url[:60]}... (–æ–∂–∏–¥–∞–Ω–∏–µ: {wait_time} —Å–µ–∫)"
                    )
            else:
                self.regular_requests += 1

            return None
        except Exception:
            return None
